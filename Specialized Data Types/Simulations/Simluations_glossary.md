# Simulations Glossary

This glossary of terms is an accompaniment to the Curating
Simulation-based Research workshop. They are terms you might see or hear
when curating with simulations and associated materials.

| **Term** | **Definition** |
|----------|----------------|
| Benchmark data  | Data that can be used for verification and validation. “Ground truth” that the simulation can be compared to. It can also be synthetic data produced via theoretical/mathematical means to ensure the computational algorithms are behaving as expected. |
| Calibration| The process of tuning the model parameters to match observed data. |
| Computational science| A field of research concerned with applying computer science and software engineering principles to problems in a particular scientific discipline. For example, “computational hydrogeology” refers to solving problems in hydrogeology (the study of water in the ground) computationally using simulation. Also referred to as Scientific Computing.|
| Discretization| A general term describing how a model is split up into smaller pieces (spatially and/or temporally) so a computer can run computations on those pieces. See Finite difference, finite element.  |
| Experiment | A particular simulation run. A set of runs with differing parameters or model characteristics is sometimes called an ensemble. |
| Finite difference, finite element    | Two ways a mathematical model (represented as a differential equation) can be discretized.   |
| Model      | A simplified representation of reality. Usually a model is expressed mathematically so it can be simulated in a computer.    |
| Numerical methods/analysis    | Algorithms and processes designed to solve mathematical equations in a computer and analyze their characteristics for the purpose of understanding how they will behave when simulating models. |
| Parameters, forcing factors, boundary conditions, initial conditions | Tunable ‘dials’ of a simulation that cause it to behave differently. |
| Partial differential equations (PDEs)| One way to mathematically model the real world.    |
| Pipeline, data pipeline, processing pipeline, workflow | A sequence of sequential computational steps executed manually or automatically (often involving different tools) that produces, transforms, processes, analyzes, data or otherwise produces an output. Data collection, cleaning, verification, storage, summarizing, output generation (e.g., via simulation), post-processing, visualization are examples of steps in a pipeline. |
| Sensitivity analysis | A process that characterizes how much influence each input parameter (and combinations thereof) have on a model’s output.    |
| Simulation | Computer approximation of reality, based on a model. |
| Stochastic simulation| A class of simulations based in statistical principles and/or random sampling (e.g., Monte Carlo methods).   |
| Synthetic data| Data that was not collected but instead was created to reflect the expected characteristics of real data that has yet to be collected or cannot be collected. Often used to test models. |
| Uncertainty analysis | A process that characterizes how a model’s output behaves when the input parameters are uncertain. Also includes characterizing the degree of certainty of the input parameters.|
| Validation | Testing to ensure the model is producing results in accordance to what might be expected in reality. I.e., ensuring one is solving the correct problem.|
| Verification  | Testing to ensure the simulation code is producing results in accordance to what is expected from the model. I.e., ensuring one is solving the problem correctly.      |
